{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from numpy import array\n",
    "import random\n",
    "random.seed( 3 )\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "\n",
    "SOURCE_URL = 'http://pwz.mit.edu/data/train1018/'\n",
    "WORK_DIRECTORY = \"/train1018\"\n",
    "\n",
    "def maybe_download(filename):\n",
    "    \"\"\"A helper to download the data files if not present.\"\"\"\n",
    "    if not os.path.exists(WORK_DIRECTORY):\n",
    "        os.mkdir(WORK_DIRECTORY)\n",
    "    filepath = os.path.join(WORK_DIRECTORY, filename)\n",
    "    if not os.path.exists(filepath):\n",
    "        filepath, _ = urlretrieve(SOURCE_URL + filename, filepath)\n",
    "        statinfo = os.stat(filepath)\n",
    "        print('Succesfully downloaded', filename, statinfo.st_size, 'bytes.')\n",
    "    else:\n",
    "        print('Already downloaded', filename)\n",
    "    return filepath  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Already downloaded', '0.txt')\n",
      "('Already downloaded', '1.txt')\n",
      "('Already downloaded', '2.txt')\n",
      "('Already downloaded', '3.txt')\n",
      "('Already downloaded', '4.txt')\n",
      "('Already downloaded', '5.txt')\n",
      "('Already downloaded', '6.txt')\n",
      "('Already downloaded', '7.txt')\n",
      "('Already downloaded', '8.txt')\n",
      "('Already downloaded', '9.txt')\n",
      "('Already downloaded', '10.txt')\n",
      "('Already downloaded', '11.txt')\n",
      "('Already downloaded', '12.txt')\n",
      "('Already downloaded', '13.txt')\n",
      "('Already downloaded', '14.txt')\n",
      "('Already downloaded', 'test2.txt')\n"
     ]
    }
   ],
   "source": [
    "data0 = maybe_download('0.txt')\n",
    "data1 = maybe_download('1.txt')\n",
    "data2 = maybe_download('2.txt')\n",
    "data3 = maybe_download('3.txt')\n",
    "data4 = maybe_download('4.txt')\n",
    "data5 = maybe_download('5.txt')\n",
    "data6 = maybe_download('6.txt')\n",
    "data7 = maybe_download('7.txt')\n",
    "data8 = maybe_download('8.txt')\n",
    "data9 = maybe_download('9.txt')\n",
    "data10 = maybe_download('10.txt')\n",
    "data11 = maybe_download('11.txt')\n",
    "data12 = maybe_download('12.txt')\n",
    "data13 = maybe_download('13.txt')\n",
    "data14 = maybe_download('14.txt')\n",
    "\n",
    "test = maybe_download('test2.txt')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182 60 30\n"
     ]
    }
   ],
   "source": [
    "with open(data0, 'r') as f0,open(data1, 'r') as f1,open(data2, 'r') as f2,open(data3, 'r') as f3,open(data4, 'r') as f4,open(data5, 'r') as f5,open(data6, 'r') as f6,open(data7, 'r') as f7,open(data8, 'r') as f8,open(data9, 'r') as f9,open(data10, 'r') as f10,open(data11, 'r') as f11,open(data12, 'r') as f12,open(data13, 'r') as f13,open(data14, 'r') as f14,open(test, 'r') as ft:\n",
    "    dd = []\n",
    "    \n",
    "    dd.append(json.load(f0))\n",
    "    dd.append(json.load(f1))\n",
    "    dd.append(json.load(f2))\n",
    "    dd.append(json.load(f3))\n",
    "    dd.append(json.load(f4))\n",
    "    dd.append(json.load(f5))\n",
    "    dd.append(json.load(f6))\n",
    "    dd.append(json.load(f7))\n",
    "    dd.append(json.load(f8))\n",
    "    dd.append(json.load(f9))\n",
    "    dd.append(json.load(f10))\n",
    "    dd.append(json.load(f11))\n",
    "    dd.append(json.load(f12))\n",
    "    dd.append(json.load(f13))\n",
    "    dd.append(json.load(f14))\n",
    "    \n",
    "    test_d = json.load(ft)\n",
    "    \n",
    "    print len(test_d),len(test_d[0]),len(test_d[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.32        0.32188691  0.32766052 ...,  2.          2.          2.        ]\n",
      " [ 0.32        0.32188691  0.32766052 ...,  2.          2.          2.        ]\n",
      " [ 0.32        0.32188691  0.32766052 ...,  2.          2.          2.        ]\n",
      " ..., \n",
      " [ 0.32        0.32188691  0.32766052 ...,  2.          2.          2.        ]\n",
      " [ 0.32        0.32188691  0.32766052 ...,  2.          2.          2.        ]\n",
      " [ 0.32        0.32188691  0.32766052 ...,  2.          2.          2.        ]]\n",
      "1800\n"
     ]
    }
   ],
   "source": [
    "    train_data = []\n",
    "    train_label = []\n",
    "    test_data = []\n",
    "\n",
    "    def combineSet(datamat,labelmat,data0,label):\n",
    "        for img in data0:\n",
    "            temp = []\n",
    "            for row in img:\n",
    "                for i in row:\n",
    "                    temp.append(i)\n",
    "            datamat.append(temp)\n",
    "            labelmat.append(label)\n",
    "\n",
    "    def labelcreator(index,total):\n",
    "        out = [0 for i in range(total)]\n",
    "        out[index]=1\n",
    "        return out\n",
    "\n",
    "    for i in range(15):\n",
    "        combineSet(train_data,train_label,dd[i],labelcreator(i,15))\n",
    "\n",
    "    def normalize(lst):\n",
    "        return [i/5000 for i in lst]\n",
    "\n",
    "    for img in test_d:\n",
    "        temp = []\n",
    "        for row in img:\n",
    "            for i in row:\n",
    "                temp.append(i)\n",
    "        test_data.append(temp)\n",
    "\n",
    "\n",
    "    #print normalize(train_data[0])\n",
    "    train_data = [normalize(i) for i in train_data]\n",
    "    test_data = [normalize(i) for i in test_data]\n",
    "\n",
    "\n",
    "    #print array(train_data)\n",
    "    print array(test_data)\n",
    "    print len(test_data[0])\n",
    "    #print len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "\n",
    "setsize = 20\n",
    "setAll = 440\n",
    "\n",
    "def subset(lst,index,num):\n",
    "    if index+num<len(lst):\n",
    "        return lst[index:index+num]\n",
    "    else:\n",
    "        lst0 = lst[index:]\n",
    "        lst1 = lst[:(num-(len(lst)-index))]\n",
    "        lst0.extend(lst1)\n",
    "        return lst0\n",
    "\n",
    "print len(subset(train_data,429,setsize))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, 1800],name = \"x\")\n",
    "\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 15],name = \"y_\")\n",
    "\n",
    "\n",
    "\n",
    "def weight_variable(shape,name):\n",
    "\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "\n",
    "  return tf.Variable(initial,name)\n",
    "\n",
    "\n",
    "\n",
    "def bias_variable(shape,name):\n",
    "\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "\n",
    "  return tf.Variable(initial,name)\n",
    "\n",
    "def conv2d(x, W):\n",
    "\n",
    "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "\n",
    "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "\n",
    "\n",
    "#1st conv\n",
    "\n",
    "\n",
    "\n",
    "W_conv1 = weight_variable([5, 5, 1, 32],name=\"W_conv1\")\n",
    "\n",
    "b_conv1 = bias_variable([32],name=\"b_conv1\")\n",
    "\n",
    "\n",
    "\n",
    "x_image = tf.reshape(x, [-1,60,30,1])\n",
    "\n",
    "\n",
    "\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#2nd conv\n",
    "\n",
    "W_conv2 = weight_variable([5, 5, 32, 64],name=\"v3\")\n",
    "\n",
    "b_conv2 = bias_variable([64],name=\"v4\")\n",
    "\n",
    "\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "\n",
    "\n",
    "#Densely Connected Layer\n",
    "\n",
    "W_fc1 = weight_variable([15 * 8 * 64, 1024],name=\"v5\")\n",
    "\n",
    "b_fc1 = bias_variable([1024],name=\"v6\")\n",
    "\n",
    "\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 15*8*64])\n",
    "\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "\n",
    "\n",
    "#Dropout\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32,name = \"keep_prob\")\n",
    "\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "\n",
    "\n",
    "#readout\n",
    "\n",
    "W_fc2 = weight_variable([1024, 15],name=\"v7\")\n",
    "\n",
    "b_fc2 = bias_variable([15],name=\"v8\")\n",
    "\n",
    "\n",
    "\n",
    "y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y_conv, y_))\n",
    "\n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "    correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "\n",
    "    \n",
    "\n",
    "    # Training process\n",
    "\n",
    "    for i in range(200):\n",
    "\n",
    "        myindex =  int(random.random()*setAll)\n",
    "\n",
    "        if i%100 == 0:\n",
    "\n",
    "            print i\n",
    "\n",
    "        batch_xs, batch_ys = array(subset(train_data,myindex,setsize)), array(subset(train_label,myindex,setsize))\n",
    "\n",
    "        train_step.run(feed_dict={x: batch_xs, y_: batch_ys, keep_prob: 0.5})\n",
    "\n",
    "    \n",
    "\n",
    "    save_path = saver.save(sess, \"/notebooks/cnnmodel.ckpt\")\n",
    "\n",
    "    print(\"Model saved in file: %s\" % save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
